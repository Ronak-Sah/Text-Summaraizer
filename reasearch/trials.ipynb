{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478255f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fc85d1-cc65-4bea-a9d1-3a953d32adca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c0343c-83b3-49ab-b4b5-529665417661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_code=\"google/pegasus-xsum\"\n",
    "model_pegasus=AutoModelForSeq2SeqLM.from_pretrained(model_code).to(device)\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015ebdea-18ec-4de0-89f9-33e0b1e4c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'summary', 'dialogue'],\n",
      "        num_rows: 14732\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'summary', 'dialogue'],\n",
      "        num_rows: 818\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'summary', 'dialogue'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nyamuda/samsum\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65bcc292-81ec-45dd-8221-a721ed0c8ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dataset[\"train\"]\n",
    "validation=dataset[\"validation\"]\n",
    "test=dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e75f25-2db3-4c53-b997-bf6f352a941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_function(example):\n",
    "    # Tokenize dialogue (input)\n",
    "    inputs = tokenizer(\n",
    "        example[\"dialogue\"], \n",
    "        max_length=512,  \n",
    "        truncation=True\n",
    "    )\n",
    "    # Tokenize summary (target)\n",
    "    targets = tokenizer(\n",
    "        example[\"summary\"], \n",
    "        max_length=512,  \n",
    "        truncation=True\n",
    "    )\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return {\n",
    "        \"input_ids\":inputs[\"input_ids\"],\n",
    "        \"attention_mask\":inputs[\"attention_mask\"],\n",
    "        \"labels\":targets[\"input_ids\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a23046-5701-4c82-9e6d-77285ff1964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.map(tokenize_function,batched=True)\n",
    "test=test.map(tokenize_function,batched=True)\n",
    "validation=validation.map(tokenize_function,batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b006ae-6399-4958-80e4-019743abf9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14732"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e144d083-acf1-497c-8708-3e029e8ebfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator=DataCollatorForSeq2Seq(tokenizer,model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b11ea-d652-4b83-939b-233f85b91307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True,\n",
    "    output_dir='pegasus-samsum',          # output directory\n",
    "    num_train_epochs=1,              # total # of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    gradient_accumulation_steps=8, \n",
    "    weight_decay=0.01             # strength of weight decay\n",
    "    # logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_pegasus,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train.select(range(10)),         # training dataset\n",
    "    eval_dataset=test,           # evaluation dataset\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df528868-523d-40d5-b440-ef9e5d40c316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronak\\anaconda3\\envs\\text\\lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=3.750455856323242, metrics={'train_runtime': 26.9456, 'train_samples_per_second': 0.371, 'train_steps_per_second': 0.074, 'total_flos': 4345483591680.0, 'train_loss': 3.750455856323242, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # clears reserved memory\n",
    "torch.cuda.reset_peak_memory_stats()  # optional: reset memory stats\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e69ad-b677-4ddb-8506-efe15c7cbf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  generate_batch_sized_chunks(list_of_elements,batch_size):\n",
    "    \"\"\"split the batches into smaller batches that we can process simultaneosly\n",
    "    Yield successive batched sized chunks from list_of_elements\"\"\"\n",
    "    for i in range(0,len(list_of_elements),batch_size):\n",
    "        yield list_of_elements[i:i+batch_size]                ## returns a iterator generator\n",
    "\n",
    "\n",
    "def calculate_metric(dataset,metric,model,tokenizer,batch_size=16,device=device,column_text=\"dialogue\",column_summary=\"summary\"):\n",
    "    X_batches=list(generate_batch_sized_chunks(dataset[column_text],batch_size))\n",
    "    y_batches=list(generate_batch_sized_chunks(dataset[column_summary],batch_size))\n",
    "\n",
    "    for X_batch,y_batch in tqdm(zip(X_batches,y_batches),total=len(X_batches)):\n",
    "        inputs=tokenizer(X_batch,max_length=512,truncation=True,padding=True,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        summaries=model_pegasus.generate(input_ids=inputs[\"input_ids\"].to(device),attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                         length_penalty=0.8,num_beams=8,max_length=128)\n",
    "\n",
    "\n",
    "        decode_summaries=[tokenizer.decode(s,skip_special_tokens=True,clean_up_tokenization_spaces=True) for s in summaries]\n",
    "\n",
    "        decode_summaries=[d.replace(\"\",\" \") for d in decode_summaries]\n",
    "\n",
    "        metric.add_batch(predictions=decode_summaries,references=y_batch)\n",
    "\n",
    "    score=metric.compute()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3f84849-f827-4b8d-b996-843b07dec5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_names=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "from evaluate import load\n",
    "rouge_metric = load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a00c9d4b-bd27-4f46-9199-a16c7660e54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:21<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 0.02424744121203537\n",
      "rouge2 0.0\n",
      "rougeL 0.02441862079497966\n",
      "rougeLsum 0.025470220298262725\n"
     ]
    }
   ],
   "source": [
    "score=calculate_metric(test[0:10],rouge_metric,model_pegasus,tokenizer,batch_size=2,\n",
    "                       device=device,column_text=\"dialogue\",column_summary=\"summary\")\n",
    "\n",
    "for r in rouge_names:\n",
    "    print(r, score[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb7c0f11-0ab9-400d-ad61-82d058e4fd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tokenizer\\\\tokenizer_config.json',\n",
       " 'Tokenizer\\\\special_tokens_map.json',\n",
       " 'Tokenizer\\\\spiece.model',\n",
       " 'Tokenizer\\\\added_tokens.json',\n",
       " 'Tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pegasus.save_pretrained(\"pegasus-samsum-model\")\n",
    "tokenizer.save_pretrained(\"Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "gen_kwargs={\"length_penalty\":0.8,\"num_beams\":8,\"max_length\":128}\n",
    "\n",
    "sample_text=\"\"\n",
    "\n",
    "pipe=pipeline(\"summaraization\",model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n",
    "\n",
    "ans=pipe(sample_text,**gen_kwargs)[0][\"summay_text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
